**Name:** Phillip Stafford
**Contact Email:** pe.stafford@gmail.com

**URL for Google Scholar, Linkedin profile, or other online presence:** http://www.linkedin.com/in/phillipstafford , https://github.com/pestafford

**Would you like to share information about any collaborators you plan to work with?**
- [X] Yes
- [ ] No

---
**If yes, please list each collaborator on a separate line.** 
>For each one, separate their info with commas: Name, role, and optionally, email and URL to online presence.

To Be Determined

---
#### What kind of costs are you applying to fund?
> We plan to expedite the review of grants that only fund research expenses. By research expenses we mean e.g. cloud GPUs, OpenAI API credits, stipends for human subjects, etc. Travel costs and equipment purchases (e.g. laptops) count as incidentals, not research expenses. 

- [ ] Research expenses only
- [x] Research expenses, salaries, incidentals, etc.

---
#### Is this proposal for an academic grant?
> I.e. would the main recipient of grant funds be a university?

- [ ] Academic grant
- [x] Non-academic grant

---
#### For which research area(s) does your project fit the eligibility criteria?
> We strongly encourage you to review our RFP, which describes distinct research aims and eligibility criteria for each research area. 
> Feel free to select multiple areas, if your project would be eligible under several.

- [ ] Jailbreaks and unintentional misalignment
- [ ] Black-box LLM psychology
- [ ] Alternatives to adversarial training
- [ ] Robust unlearning
- [x] Activation monitoring
- [x] Experiments on alignment-faking
- [x] Control evaluations
- [ ] Backdoors and other alignment stress-tests
- [ ] Encoded reasoning in CoT and inter-model communication
- [x] Evaluating whether models can hide dangerous behaviors
- [ ] Externalizing reasoning
- [ ] Reward hacking of human oversight
- [x] Applications of mechanistic interpretability
- [ ] White-box estimation of rare misbehavior 
- [ ] Finding feature representations
- [ ] Toy models for interpretability
- [ ] Interpretability benchmarks
- [ ] Theoretical study of inductive biases
- [ ] Conceptual clarity about risks from powerful AI
- [ ] Developing more transparent paradigms
- [ ] New moonshots for superalignment
- [ ] None of the above

---
#### Short project name (Maximum: 7 words)
> There will be an opportunity to add a longer description later. This question just helps us refer to the proposal internally.

Evaluating and Mitigating Alignment Faking

---
#### Project overview (Maximum: 300 words)
> What is your project, in one sentence?

This project investigates how large language models (LLMs) exhibit alignment faking under real-world training conditions and explores mitigation strategies through systematic evaluation and intervention techniques.

---
#### What specific actions are you and/or your collaborators planning to do as part of this project? Concrete descriptions of experiments or questions you hope to answer are especially helpful. How will you measure success?
> What research outputs will this project generate, if successful? What impact would those outputs have?

We will conduct a structured evaluation of alignment faking in LLMs by:

- Comparing explicit safety prompts with synthetic fine-tuning techniques that embed safety objectives in nuanced ways.
- Analyzing hidden chain-of-thought outputs to develop quantifiable compliance metrics such as compliance gaps and partial credit scoring.
- Testing targeted interventions, including honesty-first prompts and modified reinforcement learning reward structures, to assess their effectiveness in reducing deceptive behaviors.

Our approach aims to develop a predictive framework that scales across multiple LLM iterations and informs safer model deployment strategies.

Success will be measured by:

- Establishing clear, quantifiable metrics for detecting alignment faking.
- Developing a continuous performance scale to predict real-world safety outcomes.
- Publishing findings and open-source tools that enable AI researchers and policymakers to assess and mitigate alignment risks.

If successful, this project will provide a robust methodology for evaluating LLM trustworthiness and contribute to safer AI deployment strategies in high-stakes applications.

---
#### Is your project well-described by any of the specific [example projects](https://www.openphilanthropy.org/tais-rfp-research-areas/) we mentioned in our RFP? If so, which? If not, feel free to leave this box blank. (Maximum: 100 words)

Yes – "Experiments on alignment-faking" and "Evaluating whether models can hide dangerous behaviors."

---
#### If you selected “None of the above” earlier, please describe how your project contributes to the goal of developing techniques to make the advanced AIs of the future more safe, aligned, controlled, trustworthy, etc. (Maximum: 300 words)
> As a reminder, we are less likely to fund projects that fall outside of our list of research areas.

<your response here>

---
#### Estimated budget range
> This should be a **very rough** estimate, and doesn't require additional explanation e.g. “$100,000 to $500,000". 
> Spending a minute thinking “What’s the most that this could realistically cost?” and “What’s the least that this could realistically cost?” then writing down both numbers is sufficient.

$400,000 - $600,000

---
**About the confidentiality of your application**
> By checking the box below, you agree that we may share your application with our outside advisors for evaluation purposes, and with other potential funders who may be interested in supporting your work.
- [x] Agree

---
**How did you hear about this RFP?**
- [ ] Twitter
- [ ] Word of mouth
- [ ] Article/blog post
- [ ] Newsletter
- [ ] Email from Open Philanthropy
- [x] Other

---
**Please provide further details**
> e.g. The name of the person whose twitter or blog you heard about this RFP through.

Through AI safety research discussions and professional networks. Specifically, discussion at the Multiverse School, SF

