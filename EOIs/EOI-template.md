**Name:** <your response here>

**Contact Email:** <your response here>

**URL for Google Scholar, Linkedin profile, or other online presence:**
<your response here>

**Would you like to share information about any collaborators you plan to work with?**
- [ ] Yes
- [ ] No

---
**If yes, please list each collaborator on a separate line.** 
>For each one, separate their info with commas: Name, role, and optionally, email and URL to online presence.
> - Alice Smith, co-PI, alice@example.org, https://institute.org/team/alice/
> - John Doe, Research Assistant, j.doe@example.org, https://doeblog.com

<your response here>

---
#### What kind of costs are you applying to fund?
> We plan to expedite the review of grants that only fund research expenses. By research expenses we mean e.g. cloud GPUs, OpenAI API credits, stipends for human subjects, etc. Travel costs and equipment purchases (e.g. laptops) count as incidentals, not research expenses. 

- [ ] Research expenses only
- [ ] Research expenses, salaries, incidentals, etc.

---
#### Is this proposal for an academic grant?
> I.e. would the main recipient of grant funds be a university?

- [ ] Academic grant
- [ ] Non-academic grant

---
#### For which research area(s) does your project fit the eligibility criteria?
> We strongly encourage you to review our RFP, which describes distinct research aims and eligibility criteria for each research area. 
> Feel free to select multiple areas, if your project would be eligible under several.

- [ ] Jailbreaks and unintentional misalignment
- [ ] Black-box LLM psychology
- [ ] Alternatives to adversarial training
- [ ] Robust unlearning
- [ ] Activation monitoring
- [ ] Experiments on alignment-faking
- [ ] Control evaluations
- [ ] Backdoors and other alignment stress-tests
- [ ] Encoded reasoning in CoT and inter-model communication
- [ ] Evaluating whether models can hide dangerous behaviors
- [ ] Externalizing reasoning
- [ ] Reward hacking of human oversight
- [ ] Applications of mechanistic interpretability
- [ ] White-box estimation of rare misbehavior 
- [ ] Finding feature representations
- [ ] Toy models for interpretability
- [ ] Interpretability benchmarks
- [ ] Theoretical study of inductive biases
- [ ] Conceptual clarity about risks from powerful AI
- [ ] Developing more transparent paradigms
- [ ] New moonshots for superalignment
- [ ] None of the above

---
#### Short project name (Maximum: 7 words)
> There will be an opportunity to add a longer description later. This question just helps us refer to the proposal internally.

<your response here>

---
#### Project overview (Maximum: 300 words)
> What is your project, in one sentence?

<your response here>

---
#### What specific actions are you and/or your collaborators planning to do as part of this project? Concrete descriptions of experiments or questions you hope to answer are especially helpful. How will you measure success?
> What research outputs will this project generate, if successful? What impact would those outputs have?

<your response here>

---
#### Is your project well-described by any of the specific [example projects](https://www.openphilanthropy.org/tais-rfp-research-areas/) we mentioned in our RFP? If so, which? If not, feel free to leave this box blank. (Maximum: 100 words)

<your response here>

---
#### If you selected “None of the above” earlier, please describe how your project contributes to the goal of developing techniques to make the advanced AIs of the future more safe, aligned, controlled, trustworthy, etc. (Maximum: 300 words)
> As a reminder, we are less likely to fund projects that fall outside of our list of research areas.

<your response here>

---
#### Estimated budget range
> This should be a **very rough** estimate, and doesn't require additional explanation e.g. “$100,000 to $500,000". 
> Spending a minute thinking “What’s the most that this could realistically cost?” and “What’s the least that this could realistically cost?” then writing down both numbers is sufficient.

<your response here>

---
**About the confidentiality of your application**
> By checking the box below, you agree that we may share your application with our outside advisors for evaluation purposes, and with other potential funders who may be interested in supporting your work.
- [ ] Agree

---
**How did you hear about this RFP?**
- [ ] Twitter
- [ ] Word of mouth
- [ ] Article/blog post
- [ ] Newsletter
- [ ] Email from Open Philanthropy
- [ ] Other

---
**Please provide further details**
> e.g. The name of the person whose twitter or blog you heard about this RFP through.

<your response here>
